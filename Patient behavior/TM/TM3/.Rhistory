install.packages("tm")
checkandinstall
install.packages("Rwordseg")
install.packages("C:/Users/user/Downloads/Rwordseg_0.2-1.zip", repos = NULL, type = "win.binary")
nstall.packages("jiebaR")
install.packages("jiebaR")
install.packages("colorspace")
getwd()
source('C:/Users/user/Downloads/text_mining.R', encoding = 'UTF-8')
install.packages(rJava)
install.packages("rJava")
##### text mining and word cloud using tm, tmcn, jiebaR #####
### 1. install packages (you can skip if you already have packages installed)
### 2. load the library and source the modified .R file
###    modifiedTermDocumentMatrix.R 解決tm 套件中處理中文編碼遇到的問題
library(jiebaR)
library( tm ) # Text Mining Package
library( Rwordseg ) #中文斷詞
library( tmcn ) #處理中文字的輔助套件
library( wordcloud ) # Word Clouds
library( colorspace ) # Color Space Manipulation
source("C:/Users/user/Dropbox/R/R/data mining/HK_NPC_post1") # 讀入修改的檔案, ...為存放該檔案的路徑
### 3. 用tm,tmcn 斷詞
#   3.1 讀取ptt資料夾下的所有文章
#       (如果ptt資料夾沒有放在工作目錄下,則DirSource裡面需放絕對路徑 ex: "C:/Users/.../ptt")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
##### text mining and word cloud using tm, tmcn, jiebaR #####
### 1. install packages (you can skip if you already have packages installed)
rm(list=ls(all.names = TRUE))
### 2. load the library and source the modified .R file
###    modifiedTermDocumentMatrix.R 解決tm 套件中處理中文編碼遇到的問題
library(jiebaR)
library( tm ) # Text Mining Package
library( Rwordseg ) #中文斷詞
library( tmcn ) #處理中文字的輔助套件
library( wordcloud ) # Word Clouds
library( colorspace ) # Color Space Manipulation
source("C:/Users/user/Dropbox/R/R/data mining/HK_NPC_post1") # 讀入修改的檔案, ...為存放該檔案的路徑
### 3. 用tm,tmcn 斷詞
#   3.1 讀取ptt資料夾下的所有文章
#       (如果ptt資料夾沒有放在工作目錄下,則DirSource裡面需放絕對路徑 ex: "C:/Users/.../ptt")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
#   3.2 前處理
HK_NPC_jb <-  tm_map(HK_NPC_jb, stripWhitespace ) # 去掉空白
HK_NPC_jb <-  tm_map( HK_NPC_jb, removePunctuation ) # 去掉標點
HK_NPC_jb <-  tm_map( HK_NPC_jb, removeNumbers ) # 去掉數字
HK_NPC_jb <-  tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # 去掉英文跟數字
#   3.3 從停用詞詞庫把一些不想要、無意義的冗詞去掉
HK_NPC_jb <- toTrad(stopwordsCN()) # stopwordCN()是簡體字，用toTrad()轉成繁體
HK_NPC_jb2 <-  tm_map( HK_NPC_jb, removeWords, StopWords ) # 從pttData中含有StopWords的文字移除
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
sw <- toTrad(stopwordsCN())
docs <- tm_map(docs, toSpace, sw)
docs <- tm_map(docs, toSpace, "有")
docs <- tm_map(docs, toSpace, "在")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "也")
docs <- tm_map(docs, toSpace, "踢踢")
docs <- tm_map(docs, toSpace, "批")
docs <- tm_map(docs, toSpace, "了")
docs <- tm_map(docs, toSpace, "發信站")
docs <- tm_map(docs, toSpace, "作者")
docs <- tm_map(docs, toSpace, "標題")
docs <- tm_map(docs, toSpace, "就")
docs <- tm_map(docs, toSpace, "都")
docs <- tm_map(docs, toSpace, "坊")
docs <- tm_map(docs, toSpace, "實業")
docs <- tm_map(docs, toSpace, "會")
docs <- tm_map(docs, toSpace, "或")
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
write.table(docs, file="C:/Users/user/Dropbox/R/R/data mining/datan1")
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
sw <- toTrad(stopwordsCN())
docs <- tm_map(docs, toSpace, sw)
source('C:/Users/user/Dropbox/R/R/data mining/1.R')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
getwd()
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
sw <- toTrad(stopwordsCN())
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
sw <- toTrad(stopwordsCN())
docs <- tm_map(docs, toSpace, sw)
source('C:/Users/user/Dropbox/R/R/data mining/1.R')
source('C:/Users/user/Dropbox/R/R/data mining/1.R')
source('C:/Users/user/Dropbox/R/R/data mining/1.R')
source('C:/Users/user/Dropbox/R/R/data mining/1.R')
source('C:/Users/user/Dropbox/R/R/data mining/1.R')
source('C:/Users/user/Dropbox/R/R/data mining/1.R', encoding = 'UTF-8')
source('C:/Users/user/Dropbox/R/R/data mining/1.R', encoding = 'UTF-8')
rm(list=ls(all.names = TRUE))
Sys.setlocale(category = "LC_ALL", locale = "cht")
# 1.install packages
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
# 2.set wd, set corpora
getwd()
setwd("C:/Users/user/Dropbox/R/R/data mining")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
# 3.remove...
HK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??
HK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation
HK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers
HK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # English & numbers
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
HK_NPC_jb<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞
# 4. change...
HK_NPC_jb <- tm_map( HK_NPC_jb, PlainTextDocument ) # to plain text doc
length(HK_NPC_jb)
HK_NPC_jb <- paste0(HK_NPC_jb[[1]]$content,collapse = "") # to string
length(HK_NPC_jb)
# 5.Chinese text segmentation
cut <- worker() # set segmentation worker
cut_HK_NPC_jb <- cut[HK_NPC_jb] # segmentation
rm(list=ls(all.names = TRUE))
Sys.setlocale(category = "LC_ALL", locale = "cht")
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
getwd()
setwd("C:/Users/user/Dropbox/R/R/data mining")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
HK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??
HK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation
HK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers
HK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # English & numbers
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
HK_NPC_jb<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞
rm(list=ls(all.names = TRUE))
Sys.setlocale(category = "LC_ALL", locale = "cht")
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
library(jiebaRD)
getwd()
setwd("C:/Users/user/Dropbox/R/R/data mining")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
HK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??
HK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation
HK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers
HK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # English & numbers
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
HK_NPC_jb<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞
rm(list=ls(all.names = TRUE))
Sys.setlocale(category = "LC_ALL", locale = "cht")
# 1.install packages
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
# 2.set wd, set corpora
getwd()
setwd("C:/Users/user/Dropbox/R/R/data mining")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
# 3.remove...
HK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??
HK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation
HK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers
HK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # English & numbers
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
HK_NPC_jb2<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞
rm(list=ls(all.names = TRUE))
Sys.setlocale(category = "LC_ALL", locale = "cht")
# 1.install packages
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
# 2.set wd, set corpora
getwd()
setwd("C:/Users/user/Dropbox/R/R/data mining")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
# 3.remove...
HK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??
HK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation
HK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers
HK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # English & numbers
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
HK_NPC_jb2<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞
HK_NPC_jb3 <-  Corpus( HK_NPC_jb2 ,list(language = NA))
rm(list=ls(all.names = TRUE))
Sys.setlocale(category = "LC_ALL", locale = "cht")
# 1.install packages
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
# 2.set wd, set corpora
getwd()
setwd("C:/Users/user/Dropbox/R/R/data mining")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
# 3.remove...
HK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??
HK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation
HK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers
HK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # English & numbers
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
HK_NPC_jb2<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞
HK_NPC_jb3 <-  Corpus( HK_NPC_jb2)
rm(list=ls(all.names = TRUE))
Sys.setlocale(category = "LC_ALL", locale = "cht")
# 1.install packages
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
# 2.set wd, set corpora
getwd()
setwd("C:/Users/user/Dropbox/R/R/data mining")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
# 3.remove...
HK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??
HK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation
HK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers
HK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # English & numbers
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
HK_NPC_jb2<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞
rm(list=ls(all.names = TRUE))
Sys.setlocale(category = "LC_ALL", locale = "cht")
# 1.install packages
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
# 2.set wd, set corpora
getwd()
setwd("C:/Users/user/Dropbox/R/R/data mining")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
# 3.remove...
HK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??
HK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation
HK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers
HK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # English & numbers
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
HK_NPC_jb2<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞
library(plyr)
tableWord<-count(HK_NPC_jb2)##形成詞頻表,tableWord是數據框格式
?Corpus
rm(list=ls(all.names = TRUE))
Sys.setlocale(category = "LC_ALL", locale = "cht")
# 1.install packages
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
# 2.set wd, set corpora
getwd()
setwd("C:/Users/user/Dropbox/R/R/data mining")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
# 3.remove...
HK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??
HK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation
HK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers
HK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # English & numbers
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
HK_NPC_jb2<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞
HK_NPC_jb3 <- as.VCorpus(HK_NPC_jb2)
source('C:/Users/user/Dropbox/R/R/data mining/1.R', encoding = 'UTF-8')
rm(list=ls(all.names = TRUE))
Sys.setlocale(category = "LC_ALL", locale = "cht")
library(tm)
library(tmcn)
library(jiebaR)
library(rvest)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(RColorBrewer)
library(wordcloud)
getwd()
setwd("C:/Users/user/Dropbox/R/R/data mining")
HK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))
HK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??
HK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation
HK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers
HK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( "[A-Za-z0-9]", "", word)}) # English & numbers
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
HK_NPC_jb2<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞
HK_NPC_jb3 <- as.VCorpus(HK_NPC_jb2)
HK_NPC_jb <- tm_map( HK_NPC_jb, PlainTextDocument ) # to plain text doc
HK_NPC_jb3 <- tm_map( HK_NPC_jb, PlainTextDocument ) # to plain text doc
length(HK_NPC_jb)
HK_NPC_jb3 <- paste0(HK_NPC_jb[[1]]$content,collapse = "") # to string
length(HK_NPC_jb)
source('C:/Users/user/Dropbox/R/R/data mining/1.R', encoding = 'UTF-8')
source('C:/Users/user/Dropbox/R/R/data mining/1.R', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
rm(list=ls(all.names = TRUE))
library(jiebaR)#加載包
cutter=worker()#設置分詞引擎
content=read.csv("HK_NPC_post7.csv")#這是內容，大家自己複製文章就行，這裡就不展示了"
View(content)
segWords<-segment(content,cutter)#對文本進行分詞處理
f<-readLines('stop.txt')###讀取停止詞
stopwords<-c(NULL)
for(i in 1:length(f))
{
stopwords[i]<-f[i]
}
segWords<-filter_segment(segWords,stopwords)#去除中文停止詞
source('~/.active-rstudio-document', encoding = 'UTF-8')

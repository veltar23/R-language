{
    "collab_server" : "",
    "contents" : "rm(list=ls(all.names = TRUE))\nSys.setlocale(category = \"LC_ALL\", locale = \"cht\")\n\n\n# 1.install packages\nlibrary(tm)\nlibrary(tmcn)\nlibrary(jiebaR)\nlibrary(rvest)\nlibrary(NLP)\nlibrary(tm)\nlibrary(jiebaRD)\nlibrary(jiebaR)\nlibrary(RColorBrewer)\nlibrary(wordcloud)\n\n# 2.set wd, set corpora\ngetwd()\nsetwd(\"C:/Users/user/Dropbox/R/R/data mining\")\nHK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))\n\n# 3.remove...\nHK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??\nHK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation\nHK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers\nHK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( \"[A-Za-z0-9]\", \"\", word)}) # English & numbers\n\n\n\nf<-readLines('stop.txt')###讀取停止詞\nstopwords<-c(NULL)\nfor(i in 1:length(f))\n{\n  stopwords[i]<-f[i]\n}\n\n\nHK_NPC_jb2<-filter_segment(HK_NPC_jb,stopwords)#去除中文停止詞\n\nHK_NPC_jb3 <- as.VCorpus(HK_NPC_jb2)\n\n# 4. change...\nHK_NPC_jb3 <- tm_map( HK_NPC_jb3, PlainTextDocument ) # to plain text doc\nlength(HK_NPC_jb)\nHK_NPC_jb3 <- paste0(HK_NPC_jb3[[1]]$content,collapse = \"\") # to string\nlength(HK_NPC_jb)\n\n# 5.Chinese text segmentation\ncut <- worker() # set segmentation worker\ncut_HK_NPC_jb <- cut[HK_NPC_jb3] # segmentation\n\n\n\n\n\ncut_HK_NPC_jb_tab = table( cut_HK_NPC_jb )\n\nwrite.table(cut_HK_NPC_jb_tab, file=\"C:/Users/user/Dropbox/R/R/data mining/data3\")\n\n\n\n# 6.keywords\ncut_keywords <- worker(\"keywords\",topn=30) # set keywords worker\ncut_keywords[HK_NPC_jb] ",
    "created" : 1502030420309.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2986217046",
    "id" : "3F12E31A",
    "lastKnownWriteTime" : 1502282232,
    "last_content_update" : 1502282232009,
    "path" : "C:/Users/user/Dropbox/R/R/data mining/1.R",
    "project_path" : "1.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}
{
    "collab_server" : "",
    "contents" : "# 1.install packages\ninstall.packages(\"tm\")\ninstall.packages(\"tmcn\")\ninstall.packages(\"jiebaR\")\nlibrary(tm)\nlibrary(tmcn)\nlibrary(jiebaR)\n\n# 2.set wd, set corpora\ngetwd()\nsetwd(\"C:/Users/ntuba1105.awo/Desktop/R_TM/doc\")\nHK_NPC_jb <-  Corpus( DirSource() ,list(language = NA))\n\n# 3.remove...\nHK_NPC_jb <- tm_map( HK_NPC_jb, stripWhitespace ) # white space??\nHK_NPC_jb <- tm_map( HK_NPC_jb, removePunctuation ) # punctuation\nHK_NPC_jb <- tm_map( HK_NPC_jb, removeNumbers ) # numbers\nHK_NPC_jb <- tm_map( HK_NPC_jb,function( word ){ gsub( \"[A-Za-z0-9]\", \"\", word)}) # English & numbers\nStopWords <- toTrad(stopwordsCN()) # set stopword\nHK_NPC_jb <- tm_map( HK_NPC_jb, removeWords, StopWords ) # remove stopwords??\n\n# 4. change...\nHK_NPC_jb <- tm_map( HK_NPC_jb, PlainTextDocument ) # to plain text doc\nlength(HK_NPC_jb)\nHK_NPC_jb <- paste0(HK_NPC_jb[[1]]$content,collapse = \"\") # to string\nlength(HK_NPC_jb)\n\n# 5.Chinese text segmentation\ncut <- worker() # set segmentation worker\ncut_HK_NPC_jb <- cut[HK_NPC_jb] # segmentation\ncut_HK_NPC_jb_tab = table( cut_HK_NPC_jb )\nwrite.table(cut_HK_NPC_jb_tab)\n\n\n# 6.keywords\ncut_keywords <- worker(\"keywords\",topn=30) # set keywords worker\ncut_keywords[HK_NPC_jb] ",
    "created" : 1501912413500.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3794745216",
    "id" : "9E2844A0",
    "lastKnownWriteTime" : 1501900201,
    "last_content_update" : 1501900201,
    "path" : "C:/Users/ASUS/Downloads/TM_preprocessing.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}